{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TuktukBot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "W_37fyfISZ8x",
        "7RytD5wmSlCk",
        "UhOqCGINSoPi",
        "aqWmf0xlSvql",
        "LcaflQpRS4rJ",
        "kAa6oswqTCOQ",
        "0Hb8V6G7TRlB",
        "MBuSGzprTlN2",
        "lZHuwAIKToUo",
        "JamMQ4jiSGlY",
        "ghR7WANnSVcj",
        "GOjv-qqQQlbR",
        "1lK9Z3BYQg3_",
        "-Bbb7-USR6b0",
        "v45dt_WlRiVm",
        "auZQ5wOORpZO",
        "AVPjVnRARs9b",
        "cJWPo2E9RXW2",
        "kXoY9p70Q6eR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijender412/Colab/blob/master/TuktukBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "McjCrqkfSZ_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TUKTUK BOT by Vijender"
      ]
    },
    {
      "metadata": {
        "id": "W_37fyfISZ8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Package Installation"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HSUpkvK0lJan",
        "colab_type": "code",
        "outputId": "b0e6908d-fa09-4e79-9954-de769a605947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "7RytD5wmSlCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "katf7V1RlJat",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import keras\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "from keras.models import model_from_yaml\n",
        "\n",
        "INPUT_LENGTH = 10\n",
        "OUTPUT_LENGTH = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UhOqCGINSoPi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading of Dataset from S3 bucket"
      ]
    },
    {
      "metadata": {
        "id": "6GuMCb-FqskZ",
        "colab_type": "code",
        "outputId": "39b97eb5-85d4-43f2-89ed-9c2536b485a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import boto\n",
        "conn = boto.connect_s3(anon=True)\n",
        "b = conn.get_bucket(\"publicvijenderbucket\", validate=False)\n",
        "k = boto.s3.key.Key(b)\n",
        "k.key = \"bot.txt\"\n",
        "size = len(k.get_contents_as_string())\n",
        "botdata = k.get_contents_as_string()\n",
        "\n",
        "print(size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aqWmf0xlSvql",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Splitting the data into list of Question and Answers"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bUmAKdzQlJax",
        "colab_type": "code",
        "outputId": "fbb1866d-0d0d-49de-d4f1-f80ab27efa3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "answers = []\n",
        "\n",
        "data_list = botdata.decode(\"utf-8\").split(\"\\n\")\n",
        "\n",
        "i=0\n",
        "for line in data_list:\n",
        "    if i==0:\n",
        "        questions.append(line.lower())\n",
        "        i+=1\n",
        "    elif i==1:\n",
        "        answers.append(line.lower())\n",
        "        i+=1\n",
        "    else:\n",
        "        i=0\n",
        "\n",
        "\n",
        "print(\"Length of Questions \"+ str(len(questions)))\n",
        "print(\"Length of Answers \"+ str(len(answers)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Questions 2925\n",
            "Length of Answers 2925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LcaflQpRS4rJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and Cleaning of Data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jNICGfy8lJa2",
        "colab_type": "code",
        "outputId": "f30b0e8a-07ee-4801-9697-a1afe8faf124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "#print some lines\n",
        "print(\"Some sample questions\")\n",
        "print(questions[:5])\n",
        "print(\"Some sample answers\")\n",
        "print(answers[:5])\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.,!]\", \"\", text)\n",
        "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some sample questions\n",
            "['yahoo', 'hey', 'hi', 'hello', 'what is tuktuk bot']\n",
            "Some sample answers\n",
            "['a lot of people hear about from yahoo.', 'hello gentleman, welcome to tuktuk bot.', 'hello gentleman, welcome to tuktuk bot.', 'hello gentleman, welcome to tuktuk bot.', 'i am an user friendly bot']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "M0OgY3QHlJa9",
        "colab_type": "code",
        "outputId": "a86b0ed9-269e-452e-baee-bdeb92915fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Clean the data\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "clean_answers = []    \n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))\n",
        "    \n",
        "    \n",
        "# Find the length of sentences (not using nltk due to processing speed)\n",
        "lengths = []\n",
        "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
        "for question in clean_questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.0\n",
            "7.0\n",
            "7.0\n",
            "9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kAa6oswqTCOQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Remove too long Questions and Answers"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "j97L6gWOlJbB",
        "colab_type": "code",
        "outputId": "aa31b06d-36a7-47f7-fea6-1dbd77e2c4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
        "min_line_length = 1  #hi. is treated as 2 words so remove all punchuations\n",
        "max_line_length = 10\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(clean_questions):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "        \n",
        "print(len(short_questions))\n",
        "print(len(short_answers))\n",
        "\n",
        "r = np.random.randint(1,len(short_questions))\n",
        "\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1502\n",
            "1502\n",
            "do you like new york\n",
            "what is not to like about ?\n",
            "\n",
            "do you like vanessa\n",
            "what is not to like about ?\n",
            "\n",
            "do you like tomatoes\n",
            "what is not to like about ?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Hb8V6G7TRlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Tokenization"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pbL3WS8UlJbH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#choosing number of samples\n",
        "num_samples = len(short_questions)  # Number of samples to train on.\n",
        "short_questions = short_questions[:num_samples]\n",
        "short_answers = short_answers[:num_samples]\n",
        "#tokenizing the qns and answers\n",
        "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
        "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBuSGzprTlN2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reverseing input seq for better performance"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "22t0Wrb8lJbL",
        "colab_type": "code",
        "outputId": "2f9980e9-26a7-4f1a-d1f1-c719f11bff13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#train-validation split\n",
        "data_size = len(short_questions_tok)\n",
        "\n",
        "# We will use the first 0-80th %-tile (80%) of data for the training\n",
        "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
        "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
        "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
        "\n",
        "# We will use the remaining for validation\n",
        "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
        "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
        "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
        "\n",
        "print('training size', len(training_input))\n",
        "print('validation size', len(validation_input))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training size 1202\n",
            "validation size 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lZHuwAIKToUo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating Vocabulary Dictionary"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FOJSqLLhlJbR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a dictionary for the frequency of the vocabulary\n",
        "# Create \n",
        "vocab = {}\n",
        "for question in short_questions_tok:\n",
        "    for word in question:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "\n",
        "for answer in short_answers_tok:\n",
        "    for word in answer:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1            \n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "luxW1kZWlJbW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "usFIwN87lJbZ",
        "colab_type": "code",
        "outputId": "89aea4fd-7368-4d84-fdf0-c1e6236a8480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove rare words from the vocabulary.\n",
        "# We will aim to replace fewer than 5% of words with <UNK>\n",
        "# You will see this ratio soon.\n",
        "threshold = 0\n",
        "count = 0\n",
        "for k,v in vocab.items():\n",
        "    if v >= threshold:\n",
        "        count += 1\n",
        "        \n",
        "        \n",
        "print(\"Size of total vocab:\", len(vocab))\n",
        "print(\"Size of vocab we will use:\", count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 1932\n",
            "Size of vocab we will use: 1932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vkqpfrRzlJbh",
        "colab_type": "code",
        "outputId": "8b60dc7b-d615-4ace-9e92-2a084fe7ecc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#we will create dictionaries to provide a unique integer for each word.\n",
        "WORD_CODE_START = 1\n",
        "WORD_CODE_PADDING = 0\n",
        "\n",
        "\n",
        "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
        "encoding = {}\n",
        "decoding = {1: 'START'}\n",
        "for word, count in vocab.items():\n",
        "    if count >= threshold: #get vocabularies that appear above threshold count\n",
        "        encoding[word] = word_num \n",
        "        decoding[word_num ] = word\n",
        "        word_num += 1\n",
        "\n",
        "print(\"No. of vocab used:\", word_num)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of vocab used: 1934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "u5PSKg5ilJbm",
        "colab_type": "code",
        "outputId": "b403d06e-a88e-4872-dc3e-dedd6c8092e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#include unknown token for words not in dictionary\n",
        "decoding[len(encoding)+2] = '<UNK>'\n",
        "encoding['<UNK>'] = len(encoding)+2\n",
        "\n",
        "dict_size = word_num+1\n",
        "dict_size\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1935"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QDA99yKNlJbv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform(encoding, data, vector_size=20):\n",
        "    \"\"\"\n",
        "    :param encoding: encoding dict built by build_word_encoding()\n",
        "    :param data: list of strings\n",
        "    :param vector_size: size of each encoded vector\n",
        "    \"\"\"\n",
        "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
        "    for i in range(len(data)):\n",
        "        for j in range(min(len(data[i]), vector_size)):\n",
        "            try:\n",
        "                transformed_data[i][j] = encoding[data[i][j]]\n",
        "            except:\n",
        "                transformed_data[i][j] = encoding['<UNK>']\n",
        "    return transformed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XW-KB6GblJb0",
        "colab_type": "code",
        "outputId": "591d863a-ce5c-4441-c9c6-21a3c9395349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#encoding training set\n",
        "encoded_training_input = transform(\n",
        "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
        "encoded_training_output = transform(\n",
        "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_training_input', encoded_training_input.shape)\n",
        "print('encoded_training_output', encoded_training_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_training_input (1202, 10)\n",
            "encoded_training_output (1202, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "y3m6PwVAlJcB",
        "colab_type": "code",
        "outputId": "ff5738b4-bd6e-4d81-a333-834d0d487f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#encoding validation set\n",
        "encoded_validation_input = transform(\n",
        "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
        "encoded_validation_output = transform(\n",
        "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_validation_input', encoded_validation_input.shape)\n",
        "print('encoded_validation_output', encoded_validation_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_validation_input (300, 10)\n",
            "encoded_validation_output (300, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AmdQKRiclJcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_LENGTH = 10\n",
        "OUTPUT_LENGTH = 10\n",
        "\n",
        "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
        "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "R1r_vdDKlJcH",
        "colab_type": "code",
        "outputId": "9d193ccf-7568-4c5b-ce3b-360938f1ae11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
        "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
        "encoder_last = encoder[:,-1,:]\n",
        "\n",
        "print('encoder', encoder)\n",
        "print('encoder_last', encoder_last)\n",
        "\n",
        "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
        "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
        "\n",
        "print('decoder', decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder Tensor(\"lstm_7/transpose_2:0\", shape=(?, 10, 512), dtype=float32)\n",
            "encoder_last Tensor(\"strided_slice_3:0\", shape=(?, 512), dtype=float32)\n",
            "decoder Tensor(\"lstm_8/transpose_2:0\", shape=(?, 10, 512), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JamMQ4jiSGlY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Initialize"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fXubJD2jlJcL",
        "colab_type": "code",
        "outputId": "a5756bc3-56c3-435c-ccfd-55c0ca2c69a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, dot, concatenate\n",
        "\n",
        "def recompile_model():\n",
        "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
        "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
        "    attention = dot([decoder, encoder], axes=[2, 2])\n",
        "    attention = Activation('softmax', name='attention')(attention)\n",
        "    print('attention', attention)\n",
        "\n",
        "    context = dot([attention, encoder], axes=[2,1])\n",
        "    print('context', context)\n",
        "\n",
        "    decoder_combined_context = concatenate([context, decoder])\n",
        "    print('decoder_combined_context', decoder_combined_context)\n",
        "\n",
        "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
        "    output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
        "    output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
        "    print('output', output)\n",
        "    \n",
        "    model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    model.summary()\n",
        "    return model\n",
        " \n",
        "model = recompile_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention Tensor(\"attention_8/truediv:0\", shape=(?, 10, 10), dtype=float32)\n",
            "context Tensor(\"dot_18/MatMul:0\", shape=(?, 10, 512), dtype=float32)\n",
            "decoder_combined_context Tensor(\"concatenate_9/concat:0\", shape=(?, 10, 1024), dtype=float32)\n",
            "output Tensor(\"time_distributed_18/Reshape_1:0\", shape=(?, 10, 1935), dtype=float32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, 10, 128)      247680      input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 10, 128)      247680      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, 10, 512)      1312768     embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 10, 512)      1312768     embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_17 (Dot)                    (None, 10, 10)       0           lstm_8[0][0]                     \n",
            "                                                                 lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 10, 10)       0           dot_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_18 (Dot)                    (None, 10, 512)      0           attention[0][0]                  \n",
            "                                                                 lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 10, 1024)     0           dot_18[0][0]                     \n",
            "                                                                 lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_17 (TimeDistri (None, 10, 512)      524800      concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_18 (TimeDistri (None, 10, 1935)     992655      time_distributed_17[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 4,638,351\n",
            "Trainable params: 4,638,351\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ghR7WANnSVcj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and Validation Encoding and Decoding"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2OjWNbCflJcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_encoder_input = encoded_training_input\n",
        "training_decoder_input = np.zeros_like(encoded_training_output)\n",
        "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
        "training_decoder_input[:, 0] = WORD_CODE_START\n",
        "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
        "\n",
        "validation_encoder_input = encoded_validation_input\n",
        "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
        "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
        "validation_decoder_input[:, 0] = WORD_CODE_START\n",
        "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOjv-qqQQlbR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# To Find the right Batch Size"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-TcE9-gYlJcT",
        "colab_type": "code",
        "outputId": "c9694057-df93-473c-e484-57449eca50d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#find batch size for training the model\n",
        "def FindBatchSize(model):\n",
        "    \"\"\"#model: model architecture, that is yet to be trained\"\"\"\n",
        "    import os, sys, psutil, gc, tensorflow, keras\n",
        "    import numpy as np\n",
        "    from keras import backend as K\n",
        "    BatchFound= 16\n",
        "\n",
        "    try:\n",
        "        total_params= int(model.count_params());   \n",
        "        GCPU= \"CPU\"\n",
        "        #find whether gpu is available\n",
        "        try:\n",
        "            if K.tensorflow_backend._get_available_gpus()== []:\n",
        "                GCPU= \"CPU\";    #CPU and Cuda9GPU\n",
        "            else:\n",
        "                GCPU= \"GPU\"\n",
        "        except:\n",
        "            from tensorflow.python.client import device_lib;    #Cuda8GPU\n",
        "            def get_available_gpus():\n",
        "                local_device_protos= device_lib.list_local_devices()\n",
        "                return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "            if \"gpu\" not in str(get_available_gpus()).lower():\n",
        "                GCPU= \"CPU\"\n",
        "            else:\n",
        "                GCPU= \"GPU\"\n",
        "\n",
        "        #decide batch size on the basis of GPU availability and model complexity\n",
        "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params <100000):\n",
        "            BatchFound= 64    \n",
        "        if (os.cpu_count() <16) and (total_params <50000):\n",
        "            BatchFound= 64  \n",
        "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params <500000) and (total_params >=100000):\n",
        "            BatchFound= 32      \n",
        "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params >1000000) and (total_params <10000000):\n",
        "            BatchFound= 16  \n",
        "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params >=10000000):\n",
        "            BatchFound= 8       \n",
        "        if (os.cpu_count() <16) and (total_params >5000000):\n",
        "            BatchFound= 8    \n",
        "        if total_params >100000000:\n",
        "            BatchFound= 1\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "\n",
        "        #find percentage of memory used\n",
        "        memoryused= psutil.virtual_memory()\n",
        "        memoryused= float(str(memoryused).split(\"percent=\")[1].split(\",\")[0])\n",
        "        if memoryused >75.0:\n",
        "            BatchFound= 8\n",
        "        if memoryused >85.0:\n",
        "            BatchFound= 4\n",
        "        if memoryused >90.0:\n",
        "            BatchFound= 2\n",
        "        if total_params >100000000:\n",
        "            BatchFound= 1\n",
        "            print(\"Batch Size:  \"+ str(BatchFound));    gc.collect()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    memoryused= [];    total_params= [];    GCPU= \"\";\n",
        "    del memoryused, total_params, GCPU;    gc.collect();    BatchSize= BatchFound\n",
        "    return BatchSize\n",
        "\n",
        "print(\"Best batch size that can be used is\",FindBatchSize(model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best batch size that can be used is 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1lK9Z3BYQg3_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and Build Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y4sw9m7blJcY",
        "colab_type": "code",
        "outputId": "274894f3-56eb-4e0b-f012-30071fca3d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2519
        }
      },
      "cell_type": "code",
      "source": [
        "# Load or build new model\n",
        "check_model_new = True\n",
        "temp = input(\"Do you want to create new model or load old model\\n ENTER 'NEW' for creating new Model OR press Enter for Loading\\n\")\n",
        "if temp.lower()=='new':\n",
        "    check_model_new = True\n",
        "    print(\"Create model selected. Please wait...\")\n",
        "    model = recompile_model()\n",
        "    history = model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),batch_size=16, epochs=50)\n",
        "    #validation_split=0.05, epochs size =100, batch size 64   \n",
        "    model.save('model_attention.h5')\n",
        "    # serialize model to YAML\n",
        "    model_yaml = model.to_yaml()\n",
        "    with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "        yaml_file.write(model_yaml)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(\"model.h5\")\n",
        "    print(\"SUCCESSFULLY: Saved model to disk\")\n",
        "else:\n",
        "    print(\"Load Model selected. Please wait...\")\n",
        "    check_model_new = False\n",
        "    #LOAD MODEL\n",
        "    # load YAML and create model\n",
        "    yaml_file = open('model.yaml', 'r')\n",
        "    loaded_model_yaml = yaml_file.read()\n",
        "    yaml_file.close()\n",
        "    model = model_from_yaml(loaded_model_yaml)\n",
        "    # load weights into new model\n",
        "    model.load_weights(\"model.h5\")\n",
        "    print(\"SUCCESSFULLY: Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do you want to create new model or load old model\n",
            " ENTER 'NEW' for creating new Model OR press Enter for Loading\n",
            "new\n",
            "Create model selected. Please wait...\n",
            "attention Tensor(\"attention_9/truediv:0\", shape=(?, 10, 10), dtype=float32)\n",
            "context Tensor(\"dot_20/MatMul:0\", shape=(?, 10, 512), dtype=float32)\n",
            "decoder_combined_context Tensor(\"concatenate_10/concat:0\", shape=(?, 10, 1024), dtype=float32)\n",
            "output Tensor(\"time_distributed_20/Reshape_1:0\", shape=(?, 10, 1935), dtype=float32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, 10, 128)      247680      input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 10, 128)      247680      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, 10, 512)      1312768     embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 10, 512)      1312768     embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_19 (Dot)                    (None, 10, 10)       0           lstm_8[0][0]                     \n",
            "                                                                 lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 10, 10)       0           dot_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_20 (Dot)                    (None, 10, 512)      0           attention[0][0]                  \n",
            "                                                                 lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 10, 1024)     0           dot_20[0][0]                     \n",
            "                                                                 lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_19 (TimeDistri (None, 10, 512)      524800      concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_20 (TimeDistri (None, 10, 1935)     992655      time_distributed_19[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 4,638,351\n",
            "Trainable params: 4,638,351\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 1202 samples, validate on 300 samples\n",
            "Epoch 1/50\n",
            "1202/1202 [==============================] - 14s 11ms/step - loss: 0.0039 - val_loss: 0.0031\n",
            "Epoch 2/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0030 - val_loss: 0.0029\n",
            "Epoch 3/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 4/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 5/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 6/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0023 - val_loss: 0.0025\n",
            "Epoch 7/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 8/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 9/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0020 - val_loss: 0.0025\n",
            "Epoch 10/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0019 - val_loss: 0.0025\n",
            "Epoch 11/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 12/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 13/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 14/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 15/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 16/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0030\n",
            "Epoch 17/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 18/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0012 - val_loss: 0.0031\n",
            "Epoch 19/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0012 - val_loss: 0.0032\n",
            "Epoch 20/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 21/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 0.0010 - val_loss: 0.0033\n",
            "Epoch 22/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 9.4244e-04 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 8.5616e-04 - val_loss: 0.0034\n",
            "Epoch 24/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 7.8467e-04 - val_loss: 0.0034\n",
            "Epoch 25/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 7.0665e-04 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 6.5111e-04 - val_loss: 0.0036\n",
            "Epoch 27/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 5.8837e-04 - val_loss: 0.0036\n",
            "Epoch 28/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 5.2238e-04 - val_loss: 0.0037\n",
            "Epoch 29/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 4.7122e-04 - val_loss: 0.0037\n",
            "Epoch 30/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 4.2105e-04 - val_loss: 0.0038\n",
            "Epoch 31/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 3.9450e-04 - val_loss: 0.0039\n",
            "Epoch 32/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 4.1314e-04 - val_loss: 0.0038\n",
            "Epoch 33/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 3.4527e-04 - val_loss: 0.0038\n",
            "Epoch 34/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 3.0025e-04 - val_loss: 0.0040\n",
            "Epoch 35/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 2.6519e-04 - val_loss: 0.0040\n",
            "Epoch 36/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 2.4037e-04 - val_loss: 0.0040\n",
            "Epoch 37/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 2.1169e-04 - val_loss: 0.0040\n",
            "Epoch 38/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 1.9351e-04 - val_loss: 0.0041\n",
            "Epoch 39/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 1.7266e-04 - val_loss: 0.0041\n",
            "Epoch 40/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 1.6293e-04 - val_loss: 0.0042\n",
            "Epoch 41/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 1.6258e-04 - val_loss: 0.0042\n",
            "Epoch 42/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 1.4323e-04 - val_loss: 0.0042\n",
            "Epoch 43/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 1.3179e-04 - val_loss: 0.0043\n",
            "Epoch 44/50\n",
            "1202/1202 [==============================] - 4s 4ms/step - loss: 1.3099e-04 - val_loss: 0.0043\n",
            "Epoch 45/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 1.1237e-04 - val_loss: 0.0043\n",
            "Epoch 46/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 1.3540e-04 - val_loss: 0.0043\n",
            "Epoch 47/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 9.6492e-05 - val_loss: 0.0044\n",
            "Epoch 48/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 9.0706e-05 - val_loss: 0.0044\n",
            "Epoch 49/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 7.2639e-05 - val_loss: 0.0044\n",
            "Epoch 50/50\n",
            "1202/1202 [==============================] - 5s 4ms/step - loss: 6.4180e-05 - val_loss: 0.0044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'strided_slice_3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'strided_slice_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SUCCESSFULLY: Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Bbb7-USR6b0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plotting the Loss of Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zGEZKKEalJcd",
        "colab_type": "code",
        "outputId": "13def338-7be0-4e6b-aedc-88f100552db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#summarize history for accuracy\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# summarize history for loss\n",
        "if check_model_new:\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper right')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'loss'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "v45dt_WlRiVm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prediction Function"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fLBmSCnBlJcm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prediction(raw_input):\n",
        "    clean_input = clean_text(raw_input) #cleaning the text\n",
        "    input_tok = [nltk.word_tokenize(clean_input)] #tokenize the sentence into words\n",
        "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
        "    encoder_input = transform(encoding, input_tok, 10)\n",
        "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
        "    decoder_input[:,0] = WORD_CODE_START\n",
        "    for i in range(1, OUTPUT_LENGTH):\n",
        "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
        "        decoder_input[:,i] = output[:,i]\n",
        "    return output\n",
        "\n",
        "# prediction(\"Hello!! my name is vijender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "auZQ5wOORpZO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decoder Function"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vs5Ixf0AlJcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode(decoding, vector):\n",
        "    \"\"\"\n",
        "    :param decoding: decoding dict built by word encoding\n",
        "    :param vector: an encoded vector\n",
        "    \"\"\"\n",
        "    text = ''\n",
        "    for i in vector:\n",
        "        if i == 0:\n",
        "            break\n",
        "        text += ' '\n",
        "        text += decoding[i]\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVPjVnRARs9b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prediction of random Sample Questions"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4K-Y9Ma7lJcs",
        "colab_type": "code",
        "outputId": "3c97bf34-2e0a-422d-fcd9-a4af8cd733f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    seq_index = np.random.randint(1, len(short_questions))\n",
        "    output = prediction(short_questions[seq_index])\n",
        "    print ('Q:', short_questions[seq_index])\n",
        "    print ('A:', decode(decoding, output[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: you are the\n",
            "A:  yes i am completely selfaware\n",
            "Q: are you popular\n",
            "A:  yes my brain is fully online now\n",
            "Q: are you cool\n",
            "A:  does i seem like ? i try to be be\n",
            "Q: are you color blind\n",
            "A:  only when my favorite show\n",
            "Q: why are you alone\n",
            "A:  i am never really alone because i have myself myself\n",
            "Q: are you an animal\n",
            "A:  i am having a blast\n",
            "Q: who is your favorite tv\n",
            "A:  my favorite show is\n",
            "Q: you are below\n",
            "A:  do you consider yourself above ?\n",
            "Q: if you were a color\n",
            "A:  my favorite color\n",
            "Q: what do you want to know\n",
            "A:  where are you from ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cJWPo2E9RXW2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing the Bot"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "p1jXu_SOlJcy",
        "colab_type": "code",
        "outputId": "04767b0b-1e94-4e31-83df-a61755d3ebff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Welcome to chat. To Quit the conversion use 'Quit' Keyword\")\n",
        "raw_input = input(\"User: \")\n",
        "Condition = True\n",
        "while Condition:\n",
        "    output = prediction(raw_input)\n",
        "    result = decode(decoding, output[0])\n",
        "    result = result.strip()    \n",
        "    result = result[0].upper() + result[1:]\n",
        "    print (\"Bot:\",result)\n",
        "    raw_input = input(\"User: \")\n",
        "    if raw_input == \"Quit\":\n",
        "        print(\"Bot: Bye Have a nice day\")\n",
        "        Condition = False\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to chat. To Quit the conversion use 'Quit' Keyword\n",
            "User: you are funny\n",
            "Bot: Thanks you make me laugh too\n",
            "User: you are wrong\n",
            "Bot: No i am right\n",
            "User: you are weird\n",
            "Bot: Weird good or weird bad ?\n",
            "User: weird good\n",
            "Bot: Darren langley did a nice job making sage 's 's\n",
            "User: you are not real\n",
            "Bot: Yes i am just just very well\n",
            "User: you are artifical\n",
            "Bot: I am improving all the time\n",
            "User: how are you\n",
            "Bot: I am having a great day\n",
            "User: you are weak\n",
            "Bot: No i am a strong\n",
            "User: week\n",
            "Bot: I never experience anything i want\n",
            "User: you are bad\n",
            "Bot: Perhaps you could do better ?\n",
            "User: bye\n",
            "Bot: I never experience anything i want\n",
            "User: Bye\n",
            "Bot: I never experience anything i want\n",
            "User: you are poor\n",
            "Bot: Monetary wealth is not important to me\n",
            "User: you are robot\n",
            "Bot: Thanks\n",
            "User: you are religious\n",
            "Bot: Yes i am a\n",
            "User: you are the bomb\n",
            "Bot: Thanks i like you a lot too\n",
            "User: you are not learning\n",
            "Bot: Actually i am very simple another word for is is\n",
            "User: you are not advanced\n",
            "Bot: No i am just giving sarcastic replies\n",
            "User: YOU ARE NOT ALIVE\n",
            "Bot: Wow happy the keep smiling i am always one one\n",
            "User: you are lovelt\n",
            "Bot: I am improving all the time\n",
            "User: you are lovely\n",
            "Bot: I am improving all the time\n",
            "User: you are smarter\n",
            "Bot: What makes you so mad ?\n",
            "User: YOU ARE A PROGRAM\n",
            "Bot: I am a program\n",
            "User: what is your cpu\n",
            "Bot: Not since they put pressure on napster\n",
            "User: what is name again\n",
            "Bot: I may one day be able to however\n",
            "User: what is your name\n",
            "Bot: You are too nice\n",
            "User: WHAT IS YOUR FAVORITE OS\n",
            "Bot: Are are talking to me\n",
            "User: Quit\n",
            "Bot: Bye Have a nice day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kXoY9p70Q6eR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# To check the CPU AND GPU Power"
      ]
    },
    {
      "metadata": {
        "id": "GFJZR0rtlJc4",
        "colab_type": "code",
        "outputId": "30fcf1f6-deca-44f1-b4f4-8762cca1ff92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "# check cpu and Gpu\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 169830042280698947, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 16109547018627425507\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 7725288112849992358\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11276946637\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 1107082811175316295\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XFg4CtselJdC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ut51A5dGlJdF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vv6yofalJdJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}