{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_Reinforcement_Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijender412/Colab/blob/master/colab_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8lL3BU_wUl8a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple reinforcement learning\n",
        "\n",
        "This colab solves Cartpole using OpenAI's gym using vanilla policy gradients (PG) with TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "7CVtlflFXPJ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install neccessary packages\n",
        "Make sure to include X virtual framebuffer (`xvfb`), which is a display server implementing the X11 display to allow us to write our results out to a gif."
      ]
    },
    {
      "metadata": {
        "id": "K2_8tKPlFIYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig libffi-dev\n",
        "apt-get install -y xvfb\n",
        "pip install gym\n",
        "pip install gym[atari]\n",
        "pip install moviepy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2BSv6RL3Xmn1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Python packages\n"
      ]
    },
    {
      "metadata": {
        "id": "32CTUROkFZOV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files  # To download gif output.\n",
        "import imageio  \n",
        "imageio.plugins.ffmpeg.download()  # ffmpeg for imageio to write gif."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eeChKxGmiFfA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write Python to file\n",
        "Later we will call it using `xvfb`, which is neccesary because colab on running on a remote server doesn't have a display environment to display results. However we can write the results to a gif to view it."
      ]
    },
    {
      "metadata": {
        "id": "V1e85hDLGYYz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile cartpole.py\n",
        "'''Solve cartpole using policy gradients.\n",
        "\n",
        "source: https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb'''\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Inputs:\n",
        "# Num\tObservation\tMin\tMax\n",
        "# 0\tCart Position\t-2.4\t2.4\n",
        "# 1\tCart Velocity\t-Inf\tInf\n",
        "# 2\tPole Angle\t~ -41.8°\t~ 41.8°\n",
        "# 3\tPole Velocity At Tip\t-Inf\tInf\n",
        "# Outputs: Left/right\n",
        "\n",
        "n_inputs = 4  \n",
        "n_hidden = 4\n",
        "n_outputs = 1  # Left/right.\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
        "\n",
        "# Build network.\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)\n",
        "logits = tf.layers.dense(hidden, n_outputs)\n",
        "outputs = tf.nn.sigmoid(logits)  # Probability of action 0 (left).\n",
        "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
        "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
        "\n",
        "y = 1. - tf.to_float(action)\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
        "gradients = [grad for grad, variable in grads_and_vars]\n",
        "gradient_placeholders = []\n",
        "grads_and_vars_feed = []\n",
        "for grad, variable in grads_and_vars:\n",
        "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
        "    gradient_placeholders.append(gradient_placeholder)\n",
        "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
        "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted_rewards = np.zeros(len(rewards))\n",
        "    cumulative_rewards = 0\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
        "        discounted_rewards[step] = cumulative_rewards\n",
        "    return discounted_rewards\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
        "\n",
        "  \n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "n_games_per_update = 10\n",
        "n_max_steps = 1000\n",
        "n_iterations = 100\n",
        "save_iterations = 10\n",
        "discount_rate = 0.95\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for iteration in range(n_iterations):\n",
        "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
        "        all_rewards = []\n",
        "        all_gradients = []\n",
        "        for game in range(n_games_per_update):\n",
        "            current_rewards = []\n",
        "            current_gradients = []\n",
        "            obs = env.reset()\n",
        "            for step in range(n_max_steps):\n",
        "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)})\n",
        "                obs, reward, done, info = env.step(action_val[0][0])\n",
        "                current_rewards.append(reward)\n",
        "                current_gradients.append(gradients_val)\n",
        "                if done:\n",
        "                    break\n",
        "            all_rewards.append(current_rewards)\n",
        "            all_gradients.append(current_gradients)\n",
        "\n",
        "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
        "        feed_dict = {}\n",
        "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
        "            # Modulate the gradient with advantage/reward \n",
        "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
        "                                      for game_index, rewards in enumerate(all_rewards)\n",
        "                                          for step, reward in enumerate(rewards)], axis=0)\n",
        "            feed_dict[gradient_placeholder] = mean_gradients\n",
        "        sess.run(training_op, feed_dict=feed_dict)\n",
        "        if iteration % save_iterations == 0:\n",
        "            saver.save(sess, \"./my_policy_net_pg.ckpt\")\n",
        "    # Write out result at the end.\n",
        "    obs = env.reset()\n",
        "    steps = []\n",
        "    done = False\n",
        "    while not done:\n",
        "      s = env.render('rgb_array')\n",
        "      steps.append(s)\n",
        "      action_val, gradients_val = sess.run([action, gradients], \n",
        "                                           feed_dict={X: obs.reshape(1, n_inputs)})\n",
        "      obs, reward, done, info = env.step(action_val[0][0])\n",
        "      \n",
        "clip = ImageSequenceClip(steps, fps=30)\n",
        "clip.write_gif('result.gif', fps=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "udEXqVEZZKBi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train model and view results"
      ]
    },
    {
      "metadata": {
        "id": "hn217C9T-FNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "xvfb-run -s \"-screen 0 1400x900x24\" python cartpole.py\n",
        "# Copy to colab folder so we can visualize it.\n",
        "cp result.gif /usr/local/share/jupyter/nbextensions/google.colab/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9emBO7zWf33",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<img src='/nbextensions/google.colab/result.gif'/>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9uOKBGrUWBX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Alternatively, download file!\n",
        "files.download('result.gif')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wwAwVYR9ZiLC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Additional reading\n",
        "* [Deep Reinforcement Learning on GCP](https://cloud.google.com/blog/products/ai-machine-learning/deep-reinforcement-learning-on-gcp-using-hyperparameters-and-cloud-ml-engine-to-best-openai-gym-games)\n",
        "* [Scale the model to solve Atari Breakout](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/rl-on-gcp)"
      ]
    },
    {
      "metadata": {
        "id": "IvmqMRfqaxf3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}